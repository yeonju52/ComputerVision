{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wider Face Dataset\n",
    "- 32,203 images and label 393, 703 faces with a high degree of variability in scale, pose and occlusion\n",
    "- WIDER FACE dataset is organized based on 60 event classes\n",
    "- For each event class, we randomly select 40%/10%/50% data as training, validation and testing sets\n",
    "- After annotating the **face bounding boxes**, we further annotate the following\n",
    "attributes: **pose (typical, atypical)** and **occlusion level (partial, heavy)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot from 2023-04-04 17-27-29.png](../img/Screenshot%20from%202023-04-04%2017-27-29.png)\n",
    " ![Screenshot from 2023-04-04 17-34-52.png](../img/Screenshot%20from%202023-04-04%2017-34-52.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../img/Screenshot%20from%202023-04-04%2017-41-00.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall\n",
    "we define three levels of difficulty based on the detection rate of EdgeBox (model method):\n",
    "- Easy\n",
    "- Medium\n",
    "- Hard\n",
    "\n",
    "The average recall rates for these three levels are 92%, 76%, and 34%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale\n",
    "We group the faces by their image size (height in pixels) into three scales: \n",
    " - small (between 10-50 pixels)\n",
    " - medium (between 50-300 pixels)\n",
    " - large (over 300 pixels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occlusion\n",
    "(Occlusion is an important factor for evaluating the face detection performance.)\n",
    "\n",
    "(we ask annotator to measure the fraction of occlusion region for each face.)\n",
    "\n",
    "we treat occlusion as an attribute and assign faces into three categories:\n",
    "- no occlusion\n",
    "- partial occlusion (1%-30% of the total face area is occluded)\n",
    "- heavy occlusion (A face with occluded area over 30%)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pose\n",
    "\n",
    "we define two pose deformation levels:\n",
    "- typical (*The policy is below*)\n",
    "- atypical\n",
    "\n",
    "Face is annotated as atypical under two conditions: either the roll or pitch degree is larger than 30-degree; or the yaw is larger than 90-degree."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event\n",
    "- WIDER FACE contains 60 event categories covering a large number of scenes in the real world\n",
    "- To evaluate the influence of event to face detection, we characterize each event with three factors: scale, occlusion, and pose.\n",
    "- For each factor we compute the detection rate for the specific event class and then rank the detection rate in an ascending order. Based on the\n",
    "rank, events are divided into three partitions: easy (41-60 classes), medium (21-40 classes) and hard (1-20 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
